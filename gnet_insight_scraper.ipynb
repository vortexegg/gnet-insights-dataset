{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNET Insight Scraper\n",
    "\n",
    "Code to scrape GNET insight post contents and relevant metadata from posts on GNET's research blog at https://gnet-research.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import typing as t\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import json\n",
    "from os import walk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape URLs of each insight post from the Insights index pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_insight_urls(page_url: str) -> t.List[str]:\n",
    "    print(f\"Fetching: {page_url}\")\n",
    "    search_class = \"link-to-post\"\n",
    "    page = requests.get(page_url)\n",
    "    bs = BeautifulSoup(page.content, \"html.parser\")\n",
    "    elements = bs.find_all(class_=search_class)\n",
    "    href = [x[\"href\"] for x in elements]\n",
    "    href = list(dict.fromkeys(href))        # De-duplicate URLs\n",
    "    print(f\"Got {len(href)} insight URLs\")\n",
    "    return href\n",
    "\n",
    "def scrape_insight_urls(total_pages: int):\n",
    "    # Page URL format\n",
    "    # https://gnet-research.org/resources/insights/page/10\n",
    "    url_prefix = \"https://gnet-research.org/resources/insights/page/\"\n",
    "    page_range = range(1, total_pages + 1)\n",
    "    insight_urls = []\n",
    "    for n in page_range:\n",
    "        page_url = f\"{url_prefix}{n}\"\n",
    "        page_insight_urls = get_insight_urls(page_url)\n",
    "        insight_urls.extend(page_insight_urls)\n",
    "\n",
    "    insight_urls = list(dict.fromkeys(insight_urls))        # De-duplicate URLs\n",
    "    return insight_urls\n",
    "\n",
    "insight_urls = scrape_insight_urls(total_pages=18)\n",
    "\n",
    "print(len(insight_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape and extract data content from each insight post\n",
    "\n",
    "Insight data contents will be saved to individual pickle files to facilitate pausing and restarting downloads partway through the scraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content to scrape from each page:\n",
    "\n",
    "# Insight URL\n",
    "# Title\n",
    "# Authors\n",
    "# Author URLs\n",
    "# Pub date\n",
    "# Category\n",
    "# Text\n",
    "# Insight URLs\n",
    "# Tags\n",
    "\n",
    "# Get authors information\n",
    "def extract_authors(bs: BeautifulSoup):\n",
    "    author_names = []\n",
    "    author_urls = []\n",
    "    authors_elements = bs.find_all(class_=\"author url fn\")\n",
    "    for author_elem in authors_elements:\n",
    "        author_name = author_elem.contents[0]\n",
    "        author_url = author_elem[\"href\"]\n",
    "        author_names.append(author_name)\n",
    "        author_urls.append(author_url)\n",
    "    return {\"author_names\": author_names, \"author_urls\": author_urls}\n",
    "\n",
    "def extract_title(bs: BeautifulSoup):\n",
    "    title_elem = bs.find(class_=\"entry-title\")\n",
    "    title = title_elem.contents[0]\n",
    "    title = title.replace('\\xa0', ' ').strip()\n",
    "    return {\"title\": title}\n",
    "\n",
    "def extract_pub_date(bs):\n",
    "    date_elem = bs.find(\"time\")\n",
    "    pub_date = date_elem[\"datetime\"]\n",
    "    return {\"pub_date\": pub_date}\n",
    "\n",
    "def extract_categories(bs: BeautifulSoup):\n",
    "    categories_elem = bs.find(class_=\"categories\")\n",
    "    categories = categories_elem.find_all(\"a\")\n",
    "    cats = []\n",
    "    for cat_elem in categories:\n",
    "        category = cat_elem.contents[0]\n",
    "        cats.append(category)\n",
    "    return {\"categories\": cats}\n",
    "\n",
    "def extract_text_and_urls(bs: BeautifulSoup):\n",
    "    insight_text = []\n",
    "    insight_urls = []\n",
    "\n",
    "    mailmunch_elem = bs.find(class_=\"mailmunch-forms-before-post\")\n",
    "    for sib in mailmunch_elem.next_siblings:\n",
    "        if sib.has_attr('class') and 'mailmunch-forms-after-post' in sib[\"class\"]:\n",
    "            break\n",
    "\n",
    "        text_string = sib.get_text()\n",
    "        text_string = text_string.replace('\\xa0', ' ').strip()\n",
    "        insight_text.append(text_string)\n",
    "        hrefs = sib.findChildren(\"a\")\n",
    "        for href in hrefs:\n",
    "            if href.has_attr('href'):\n",
    "                insight_urls.append(href[\"href\"])\n",
    "\n",
    "    return {\"insight_text\": insight_text, \"insight_urls\": insight_urls}\n",
    "\n",
    "def extract_tags(bs):\n",
    "    tags = []\n",
    "    tags_element = bs.find(class_=\"meta-info-container\")\n",
    "    if (tags_element is not None):\n",
    "        anchors = tags_element.find_all(\"a\")\n",
    "        for a in anchors:\n",
    "            tags.append(a.string)\n",
    "    return {\"tags\": tags}\n",
    "\n",
    "def scrape_insight_data(insight_url: str):\n",
    "    print(f\"Scraping URL: {insight_url}\")\n",
    "    data_map = {\"insight_url\": insight_url}\n",
    "    page = requests.get(insight_url)\n",
    "    bs = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    title_info = extract_title(bs)\n",
    "    author_info = extract_authors(bs)\n",
    "    pub_date_info = extract_pub_date(bs)\n",
    "    category_info = extract_categories(bs)\n",
    "    text_info = extract_text_and_urls(bs)\n",
    "    tags_info = extract_tags(bs)\n",
    "\n",
    "    data_map.update(title_info)\n",
    "    data_map.update(author_info)\n",
    "    data_map.update(pub_date_info)\n",
    "    data_map.update(category_info)\n",
    "    data_map.update(text_info)\n",
    "    data_map.update(tags_info)\n",
    "    return data_map\n",
    "\n",
    "def write_data_file(data):\n",
    "    import os\n",
    "    directory = \"data/\"\n",
    "    os.makedirs(directory, exist_ok = True)\n",
    "    title = data[\"title\"]\n",
    "    title = title.replace(\"/\",\"_\")\n",
    "    timestamp = round(datetime.now().timestamp())\n",
    "    filename = f\"{directory}{title}_{timestamp}.pickle\"\n",
    "    outfile = open(filename, 'wb')\n",
    "    json_data = json.dumps(data)\n",
    "    pickle.dump(json_data, outfile)\n",
    "    outfile.close()\n",
    "    print(f\"Wrote data to file: {filename}\")\n",
    "\n",
    "for i in range(1, len(insight_urls)):\n",
    "    insight_url = insight_urls[i]    \n",
    "    data = scrape_insight_data(insight_url)\n",
    "    write_data_file(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the saved pickle files into a dataframe, do initial data cleanup, and save to the dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for (dirpath, dirnames, filenames) in walk(\"data/\"):\n",
    "    files.extend(filenames)\n",
    "    break\n",
    "\n",
    "insight_data = []\n",
    "for filename in files:\n",
    "    infile = open(f\"data/{filename}\", 'rb')\n",
    "    pickle_data = pickle.load(infile)\n",
    "    data = json.loads(pickle_data)\n",
    "    insight_data.append(data)\n",
    "\n",
    "df = pd.DataFrame(insight_data)\n",
    "df[\"pub_date\"] = pd.to_datetime(df[\"pub_date\"])\n",
    "df[\"insight_text\"] = df[\"insight_text\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "df.to_csv(\"gnet_insights.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
